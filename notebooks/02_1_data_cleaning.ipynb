{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1996e09",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Now we will proceed to the data cleanin phase, the raw data that we have is not quite ready for analysis just yet. We have a couple of issues:\n",
    "1) Nested structures\n",
    "2) Duplicates\n",
    "3) Missing cells\n",
    "4) unfriendly names\n",
    "\n",
    "If you have taken a peek at the `ndjson` generated in the extraction process one record may look like this:\n",
    "```json\n",
    "{\n",
    "    \"id\": \"id\",\n",
    "    \"snippet\": {\n",
    "        \"channelId\": \"channelId\",\n",
    "        \"textDisplay\": \"textDisplay\",\n",
    "        \"authorDisplayName\": \"authorDisplayName\",\n",
    "        \"authorChannelId\": {\n",
    "            \"value\": \"value\"\n",
    "            },\n",
    "        \"likeCount\": 0,\n",
    "        \"publishedAt\": \"2021-06-15T10:57:22Z\"\n",
    "        },\n",
    "    \"totalReplyCount\": 0, // missing in replies\n",
    "    \"videoId\": \"videoId\", // missing in replies\n",
    "    \"parentId\": \"parentId\" // missing in top level comments\n",
    "}\n",
    "```\n",
    "Replies for example, are missing the `videoId` field, and have an additional column for `parentId`, the top level comment that the current reply belongs to.\n",
    "We will address all of these issues in the current notebook, and save our clean dataset to a parquet file, a friendly format to handle tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2106037",
   "metadata": {},
   "source": [
    "## Imports and preliminary work\n",
    "Similar to the previous notebook, we have to make some imports and configure our notebook so it can read all our files from the project. Here, we will be using utility functions from `src/preprocessing.py`, functions that will allow us to make some analysis further on easier, by preprocessing the text.\n",
    "\n",
    "We will also work with channel handle in this notebook, so make sure that the `channel_handle` variable is configured in `cofing.py` at the root of the current project. The Paths will also provide some dinamyc pathing based on the channel handle and the date to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac1167f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with the channel handle: kurzgesagt.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datetime import date\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "# load project directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.preprocessing import *\n",
    "from paths import Paths\n",
    "import config\n",
    "\n",
    "print(f\"Working with the channel handle: {config.channel_handle}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad22ae2",
   "metadata": {},
   "source": [
    "By default, we are passing today's date to the `Paths` class, but you can change to perform cleaning on a file from a different day. You can give the specific date like\n",
    "```python \n",
    "date_path = date(2025, 9, 7) # Year, Month, day\n",
    "```\n",
    "where the first number is the year, the second the month and the third the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5af5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_path = date.today()\n",
    "channel_paths = Paths(channel_handle=config.channel_handle, date_obj=date_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af00fa",
   "metadata": {},
   "source": [
    "## Incorrect tabular format\n",
    "The objects that are in the `ndjson` file from extraction have a `json` format. But for this notebook we will need our data to be strictly tabular. Let's take a look when we load directly the `ndjson` to a polars dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a07760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>snippet</th><th>totalReplyCount</th><th>videoId</th></tr><tr><td>str</td><td>struct[7]</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;UghQJ7ZtN3UTlXgCoAEC&quot;</td><td>{&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;,&quot;omg so amazing&quot;,null,&quot;@adilsonfilho9622&quot;,{&quot;UCU0pfz9gGQOhL5-OV233bag&quot;},0,&quot;2016-08-10T06:20:05Z&quot;}</td><td>0</td><td>&quot;ZW3aV7U-aik&quot;</td></tr><tr><td>&quot;UggDV5rwA11cPXgCoAEC&quot;</td><td>{&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;,&quot;I thought newtron stars where made of single newtrons...\n",
       "\n",
       "CLICKBAIT!!!&quot;,null,&quot;@mememachine1392&quot;,{&quot;UCwKJl2Wl6nd8maT-INzMh7g&quot;},0,&quot;2016-08-09T06:50:38Z&quot;}</td><td>0</td><td>&quot;ZW3aV7U-aik&quot;</td></tr><tr><td>&quot;Ugj9OA7MtJ7hSHgCoAEC&quot;</td><td>{&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;,&quot;Im confused, so when a star dies it supernovas and then always becomes a neutron star? Or is neutron star only one option for a supernova&#x27;d star&quot;,null,&quot;@kaitlyngardner4478&quot;,{&quot;UCvOD2fgEyjA3Wf81_02A5Yg&quot;},0,&quot;2016-08-08T02:32:05Z&quot;}</td><td>4</td><td>&quot;ZW3aV7U-aik&quot;</td></tr><tr><td>&quot;Ugj9OA7MtJ7hSHgCoAEC.8HIgs_KB8…</td><td>{&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;,&quot;It&#x27;s one possibility. It mostly depends on how big the star was when it stopped fusing things in its core. It&#x27;s really quite fascinating stuff, I end up getting stuck reading about what we know of stars&#x27; lives a lot xD&quot;,&quot;Ugj9OA7MtJ7hSHgCoAEC&quot;,&quot;@WindsorMason&quot;,{&quot;UC1IWy8L9PQ0xPm9fbGucAqA&quot;},0,&quot;2016-08-08T03:07:32Z&quot;}</td><td>null</td><td>null</td></tr><tr><td>&quot;Ugj9OA7MtJ7hSHgCoAEC.8HIgs_KB8…</td><td>{&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;,&quot;A no-so-big star (like our sun) becomes a white dwarf, a bigger star becomes a neutron star, a even bigger star becomes a black hole. :-)&quot;,&quot;Ugj9OA7MtJ7hSHgCoAEC&quot;,&quot;@Matamune87&quot;,{&quot;UCSGOUpcTk2sVg17j0p_tyJg&quot;},0,&quot;2016-08-08T21:36:54Z&quot;}</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────────────────────────┬────────────────────────────────┬─────────────────┬─────────────┐\n",
       "│ id                              ┆ snippet                        ┆ totalReplyCount ┆ videoId     │\n",
       "│ ---                             ┆ ---                            ┆ ---             ┆ ---         │\n",
       "│ str                             ┆ struct[7]                      ┆ i64             ┆ str         │\n",
       "╞═════════════════════════════════╪════════════════════════════════╪═════════════════╪═════════════╡\n",
       "│ UghQJ7ZtN3UTlXgCoAEC            ┆ {\"UCsXVk37bltHxD1rDPwtNM8Q\",\"o ┆ 0               ┆ ZW3aV7U-aik │\n",
       "│                                 ┆ …                              ┆                 ┆             │\n",
       "│ UggDV5rwA11cPXgCoAEC            ┆ {\"UCsXVk37bltHxD1rDPwtNM8Q\",\"I ┆ 0               ┆ ZW3aV7U-aik │\n",
       "│                                 ┆ …                              ┆                 ┆             │\n",
       "│ Ugj9OA7MtJ7hSHgCoAEC            ┆ {\"UCsXVk37bltHxD1rDPwtNM8Q\",\"I ┆ 4               ┆ ZW3aV7U-aik │\n",
       "│                                 ┆ …                              ┆                 ┆             │\n",
       "│ Ugj9OA7MtJ7hSHgCoAEC.8HIgs_KB8… ┆ {\"UCsXVk37bltHxD1rDPwtNM8Q\",\"I ┆ null            ┆ null        │\n",
       "│                                 ┆ …                              ┆                 ┆             │\n",
       "│ Ugj9OA7MtJ7hSHgCoAEC.8HIgs_KB8… ┆ {\"UCsXVk37bltHxD1rDPwtNM8Q\",\"A ┆ null            ┆ null        │\n",
       "│                                 ┆ …                              ┆                 ┆             │\n",
       "└─────────────────────────────────┴────────────────────────────────┴─────────────────┴─────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_df = pl.read_ndjson(channel_paths.raw_comments_file_path, n_rows=5)\n",
    "preview_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a1b1bb",
   "metadata": {},
   "source": [
    "The dataframe has four columns: `id`, `snippet`, `totalReplyCount` and `videoId`.\n",
    "For now our problem is the `snippet` column, is of type `struct`. In order to keep working with the dataframe we would need to unnest the `struct` type, and unnest any inner `struct` columns.\n",
    "We could keep working with with the current dataframe, but to make our lives easier, we will reload the dataframe again with another function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d4f34",
   "metadata": {},
   "source": [
    "### Flatten on load\n",
    "We can use the function `pl.json_normalize` to completely flatten the data on load, this will unnest or undo any `json` levels and any deeper fields will be represented like `firstLevelField.secondLevelField.field`. We load the ndjson into an array first since we can't flatten exactly at load. Instead we will use the function `json_normalize` once the data is fully loaded into array. ⚠️ <span style=\"color: orange; font-weight: bold;\">Be careful, since `pl.json_normalize` is an unstable function (according to polars documentation)</span>. If you run into any problems you can try `pandas.json_normalize` and convert the resulting dataframe from `pandas` to `polars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae14e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(channel_paths.raw_comments_file_path, 'r') as comments:\n",
    "    data = [json.loads(comment) for comment in comments]\n",
    "\n",
    "today_df = pl.json_normalize(data)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396cc74",
   "metadata": {},
   "source": [
    "## Forward Fill videoID\n",
    "As indicated in the introduction, the replies lack videoId, so in the dataframe they simply show as nulls. But a reply technically does have a videoId, it was simply not there at the extraction process. (The `videoId` for a comment doesn't make part of a `comments` resource in the YouTube Data API v3, if is there for top level comments, is because the `commentThreads` resource, the one responsible for top level comments, does contain `videoId`). Just before deduplication we will perform forward fill `videoId`. Since the data is loaded:\n",
    "1) top level comment\n",
    "2) any replies for top level comment\n",
    "\n",
    "we can be sure that a reply has the top level comment it belongs to immediately above it (or above other replies for the same top level comment). So we can perform forward fill safely, the program will take the first non null `videoId` and propagate it forward (or to the records bellow), if it finds another `videoId` it will propagate the new one found.\n",
    "Note that if the data is somehow reordered, this can lead to unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df = today_df.with_columns([\n",
    "    pl.col(\"videoId\").forward_fill().alias(\"videoId\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c26dd7",
   "metadata": {},
   "source": [
    "## Deduplication\n",
    "There is a chance of duplicates in the same day of data extraction. Duplicates are not useful for us here, they only indicate that something went wrong at insertion, but never hurts to check, even when we trust in the extraction process. We will perform two deduplication, one for local duplicates, that is, the duplicates that are in the current file. Another duplication will be global deduplication, and for that we will need information about datasets previously processed.\n",
    "\n",
    "For local deduplication we can trust that the column `id` is the key for that row. If two rows in the dataset share the same `id` we can be sure that the rest of the information for both is going to be exactly the same, so we will deduplicate via the `id` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f93988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing a total of 2586 local duplicates.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Removing a total of {today_df.is_duplicated().sum()} local duplicates.\")\n",
    "today_df = today_df.unique(subset=[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d792a94",
   "metadata": {},
   "source": [
    "### Global duplicates\n",
    "This process involves checking that we don't process comments that we have processed in previous days. For this reason we will fetch all `comment_id`s from all available clean datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ebfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_files = channel_paths.list_processed_files()  # Sorted list of parquet files\n",
    "previous_ids_df = None\n",
    "previous_ids = None\n",
    "\n",
    "if processed_files:\n",
    "    # Read only the 'id' column from all files and concatenate\n",
    "    previous_ids = [\n",
    "        pl.read_parquet(file, columns=[\"comment_id\"]) for file in processed_files\n",
    "    ]\n",
    "    previous_ids_df = pl.concat(previous_ids).unique()\n",
    "else:\n",
    "    previous_ids_df = pl.DataFrame({\"comment_id\": pl.Series(\"comment_id\", [], pl.Utf8)})\n",
    "\n",
    "del previous_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771419b",
   "metadata": {},
   "source": [
    "With all the `comment_id`s at our disposal, we will proceed and perform an `antijoin`. An `antijoin` selects all the data of the first dataset THAT ARE NOT in the second dataset, and that is exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04701b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing a total of 22 global duplicates.\n"
     ]
    }
   ],
   "source": [
    "# count gobal duplicates\n",
    "removed_rows_count = today_df.join(\n",
    "    previous_ids_df,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"comment_id\",\n",
    "    how=\"semi\"\n",
    ").height\n",
    "\n",
    "print(f\"Removing a total of {removed_rows_count} global duplicates.\")\n",
    "\n",
    "# remove global duplicates\n",
    "today_df = today_df.join(\n",
    "    previous_ids_df,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"comment_id\",\n",
    "    how=\"anti\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b9f9d",
   "metadata": {},
   "source": [
    "## Renaming & Re-typing\n",
    "Now we will worry about the names of our variables and their corresponding types. Polars did a good job at infering the types of the data, mostly. There are a couple of issues\n",
    "with the dates still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e0dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('id', String),\n",
       "        ('totalReplyCount', Int64),\n",
       "        ('videoId', String),\n",
       "        ('snippet.channelId', String),\n",
       "        ('snippet.textDisplay', String),\n",
       "        ('snippet.authorDisplayName', String),\n",
       "        ('snippet.authorChannelId.value', String),\n",
       "        ('snippet.likeCount', Int64),\n",
       "        ('snippet.publishedAt', String),\n",
       "        ('snippet.parentId', String)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add905b",
   "metadata": {},
   "source": [
    "We start by removing the names that the normalization caused to our data, remove the `snippet.` prefix, and the `.value` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cacfc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(name: str) -> str:\n",
    "    return name.replace(\"snippet.\", \"\").replace(\".value\", \"\")\n",
    "\n",
    "today_df = today_df.rename({col: clean_column_names(col) for col in today_df.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed67dbe",
   "metadata": {},
   "source": [
    "Some python friendly renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3028df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    \"id\": \"comment_id\",\n",
    "    \"videoId\": \"video_id\",\n",
    "    \"channelId\": \"channel_id\",\n",
    "    \"totalReplyCount\": \"reply_count\",\n",
    "    \"textDisplay\": \"comment\",\n",
    "    \"authorDisplayName\": \"author\",\n",
    "    \"authorChannelId\": \"author_id\",\n",
    "    \"likeCount\": \"likes\",\n",
    "    \"publishedAt\": \"published_at\",\n",
    "    \"parentId\": \"parent_id\"\n",
    "}\n",
    "\n",
    "today_df = today_df.rename(rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4d245",
   "metadata": {},
   "source": [
    "Now we fix the date for `published_at`, we set the time zone for \"Zulu\" which is a synonim for \"UTC\" since all our dates contain time zone info (the `z` at the end of the string).\n",
    "We will fill with 0 the `reply_count`, a reply by default lacks that column, so we fill with 0.\n",
    "\n",
    "(You may wonder if YouTube handles replies for replies, but that is not the case as of for now. You can only post a reply for a top level comment and if you need to \"respond\" to a reply, users usually tag the user they are responding to, eg. \"@user that is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01023fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df = today_df.with_columns([\n",
    "    pl.col(\"published_at\").str.to_datetime(time_unit=\"ms\", time_zone=\"Zulu\").alias(\"published_at\")\n",
    "])\n",
    "\n",
    "today_df = today_df.with_columns([\n",
    "    pl.col(\"reply_count\").fill_null(0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75feba59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>comment_id</th><th>reply_count</th><th>video_id</th><th>channel_id</th><th>comment</th><th>author</th><th>author_id</th><th>likes</th><th>published_at</th><th>parent_id</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;300508&quot;</td><td>300508.0</td><td>&quot;300508&quot;</td><td>&quot;300508&quot;</td><td>&quot;300508&quot;</td><td>&quot;300508&quot;</td><td>&quot;300508&quot;</td><td>300508.0</td><td>&quot;300508&quot;</td><td>&quot;160633&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;139875&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>0.534538</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.635491</td><td>&quot;2019-01-06 22:53:13.401000+00:…</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>5.927343</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>178.873217</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;Ugg--0aepx3adXgCoAEC&quot;</td><td>0.0</td><td>&quot;21eFwbb48sE&quot;</td><td>&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;</td><td>&quot;&quot;</td><td>&quot;&quot;</td><td>&quot;UC--1tqVJ3uxVrl4EkEhT3xQ&quot;</td><td>0.0</td><td>&quot;2013-07-11 15:53:10+00:00&quot;</td><td>&quot;Ugg-2ZMgvgmpX3gCoAEC&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>&quot;2016-06-06 15:11:09+00:00&quot;</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>&quot;2018-11-21 04:10:27+00:00&quot;</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>&quot;2021-03-11 09:15:05+00:00&quot;</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;UgzzzsC0zqAUii_PKzZ4AaABAg&quot;</td><td>652.0</td><td>&quot;zQGOcOUBi6s&quot;</td><td>&quot;UCsXVk37bltHxD1rDPwtNM8Q&quot;</td><td>&quot;𥿽&quot;</td><td>&quot;@희망의요정&quot;</td><td>&quot;UCzzzoJBD8L5LQONWNh2sciA&quot;</td><td>44756.0</td><td>&quot;2025-09-10 16:06:02+00:00&quot;</td><td>&quot;Ugzzzh2HzhsnmA4AKMl4AaABAg&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ comment_i ┆ reply_cou ┆ video_id  ┆ … ┆ author_id ┆ likes     ┆ published ┆ parent_i │\n",
       "│ ---       ┆ d         ┆ nt        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ _at       ┆ d        │\n",
       "│ str       ┆ ---       ┆ ---       ┆ str       ┆   ┆ str       ┆ f64       ┆ ---       ┆ ---      │\n",
       "│           ┆ str       ┆ f64       ┆           ┆   ┆           ┆           ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 300508    ┆ 300508.0  ┆ 300508    ┆ … ┆ 300508    ┆ 300508.0  ┆ 300508    ┆ 160633   │\n",
       "│ null_coun ┆ 0         ┆ 0.0       ┆ 0         ┆ … ┆ 0         ┆ 0.0       ┆ 0         ┆ 139875   │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ 0.534538  ┆ null      ┆ … ┆ null      ┆ 6.635491  ┆ 2019-01-0 ┆ null     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 6 22:53:1 ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 3.401000+ ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 00:…      ┆          │\n",
       "│ std       ┆ null      ┆ 5.927343  ┆ null      ┆ … ┆ null      ┆ 178.87321 ┆ null      ┆ null     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ 7         ┆           ┆          │\n",
       "│ min       ┆ Ugg--0aep ┆ 0.0       ┆ 21eFwbb48 ┆ … ┆ UC--1tqVJ ┆ 0.0       ┆ 2013-07-1 ┆ Ugg-2ZMg │\n",
       "│           ┆ x3adXgCoA ┆           ┆ sE        ┆   ┆ 3uxVrl4Ek ┆           ┆ 1 15:53:1 ┆ vgmpX3gC │\n",
       "│           ┆ EC        ┆           ┆           ┆   ┆ EhT3xQ    ┆           ┆ 0+00:00   ┆ oAEC     │\n",
       "│ 25%       ┆ null      ┆ 0.0       ┆ null      ┆ … ┆ null      ┆ 0.0       ┆ 2016-06-0 ┆ null     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 6 15:11:0 ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 9+00:00   ┆          │\n",
       "│ 50%       ┆ null      ┆ 0.0       ┆ null      ┆ … ┆ null      ┆ 0.0       ┆ 2018-11-2 ┆ null     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 1 04:10:2 ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 7+00:00   ┆          │\n",
       "│ 75%       ┆ null      ┆ 0.0       ┆ null      ┆ … ┆ null      ┆ 1.0       ┆ 2021-03-1 ┆ null     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 1 09:15:0 ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 5+00:00   ┆          │\n",
       "│ max       ┆ UgzzzsC0z ┆ 652.0     ┆ zQGOcOUBi ┆ … ┆ UCzzzoJBD ┆ 44756.0   ┆ 2025-09-1 ┆ Ugzzzh2H │\n",
       "│           ┆ qAUii_PKz ┆           ┆ 6s        ┆   ┆ 8L5LQONWN ┆           ┆ 0 16:06:0 ┆ zhsnmA4A │\n",
       "│           ┆ Z4AaABAg  ┆           ┆           ┆   ┆ h2sciA    ┆           ┆ 2+00:00   ┆ KMl4AaAB │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Ag       │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570345b2",
   "metadata": {},
   "source": [
    "## Derived Columns\n",
    "Now that our dataframe is mostly in place, we will derive some columns. Our scripts from `src/preprocessing.py` will be handy here.\n",
    "We will derive the following columns:\n",
    "- is_reply, for comments that are replies = True\n",
    "- comment_length, length of the comment text\n",
    "- word_count, count of the words in comment text\n",
    "- script, variable to detect the script of the text, for now it only works with `latin` and `korean`\n",
    "- comment_emojis, list of emojis from the comment text\n",
    "- emoji_count, count of emojis in comment text (or length of the previous column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66247f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df = today_df.with_columns([\n",
    "    # 1. Is it a reply? (parent_id not null)\n",
    "    pl.col(\"parent_id\").is_not_null().alias(\"is_reply\"),\n",
    "\n",
    "    # 2. Comment length in characters\n",
    "    pl.col(\"comment\").str.len_chars().alias(\"comment_length\"),\n",
    "\n",
    "    # 3. Word count (count non-space sequences)\n",
    "    pl.col(\"comment\").str.count_matches(r\"\\S+\").alias(\"word_count\"),\n",
    "\n",
    "    # 4. Script detection\n",
    "    pl.col(\"comment\").map_elements(detect_script).alias(\"script\"),\n",
    "\n",
    "    # 5. Extract emojis\n",
    "    pl.col(\"comment\").map_elements(extract_emojis, return_dtype=pl.List(pl.Utf8)).alias(\"comment_emojis\"),\n",
    "])\n",
    "\n",
    "# 6. Count emojis (length of list column)\n",
    "today_df = today_df.with_columns([\n",
    "    pl.col(\"comment_emojis\").list.len().alias(\"emoji_count\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09e1da",
   "metadata": {},
   "source": [
    "## Saving our clean file\n",
    "Now we are almost done, we will save our clean file to parquet. Our handy `Paths` class will provide us with the object that has the correct path for the clean file for a specific YouTube channel and specific day. `paths.clean_comments_file` holds the correct path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d025b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df.write_parquet(channel_paths.clean_comments_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245058e",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "- **Why is the notebook using polars?, why not pandas?**\n",
    "\n",
    "A: In the development of this notebook there were several concerns regarding the use of memory by pandas, sometimes reaching up to 95% in a 16GB system. The decision to split the analyses into different days and using polars were part of the solution. Polars specially is incredibly memory friendly and can use parallelization with CPU. Not only memory friendly, but also faster. Not to mention that polars was written in Rust.\n",
    "\n",
    "- **Why deduplicate after performing the forward fill?**\n",
    "\n",
    "A: There were some unnexpected behaviors wen we deduplicated first and then performed the forward fill later. Like replies without a top level comment.\n",
    "\n",
    "- **What is the purpose of `del` and `gc.collect()`?**\n",
    "\n",
    "A: To free memory explicitly. The files handled here can be somewhat large for relatively low memory systems, so, if we don't need an object anymore we simply delete the refferences to it and force python to run the garbage collection process (since `del` only removes the pointers to an object, but sometimes the memory is not freed immediately).\n",
    "\n",
    "- **Why latin script and Korean?**\n",
    "\n",
    "A: One of the first channels that were tested with the current notebook had numerous Korean comments (specially Korean). The process of tokenization for the Korean language is different for that of latin languages, so the tokenizer checks the script first, if it is hangul it runs the Korean tokenizer, if it is not, it runs the latin tokenizer. The performace of the Korean tokenizer remains to be tested, due to the lack of Korean speakers during the development. Support for more languages and scripts (like Japanese, Chinese, Hindi, Arabic, etc) may be added in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
