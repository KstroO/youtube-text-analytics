{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b20f13",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "All the heavy lifting for data extraction happens here. We make use of some functions declared in `src/data_acquisition.py` to avoid bloating the notebook. The functions over in `data_acquisition.py` use the `API_KEY` defined in `.env` at the root of the current directory. If the `API_KEY` is not there the code may fail.\n",
    "\n",
    "The current notebook extracts the following:\n",
    "1)  Imports and Preliminary work\n",
    "2)  Extract YouTube Channel ID based on handle\n",
    "3)  Extract YouTube Uploads Playlist based on Channel ID\n",
    "4)  Extract YouTube Videos based on YouTube Playlist ID\n",
    "5)  Extract YouTube Comments & Replies based on YouTube Video List\n",
    "6)  (Optional) Extract YouTube Playlists based on Channel ID\n",
    "\n",
    "If you are continuing the fetch of comments from `<channel_handle>_videos.json`, there is no need to do steps 2, 3, 4 or 6. Executing 1 and 5 is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d7e08",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "- Remember to configure your desired channel handle in `config.py` so that other notebooks have also access to the same YouTube handle. It is important to identify the data produced by the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f8cf4",
   "metadata": {},
   "source": [
    "## 1) Imports and Preliminary Work\n",
    "We need some basic libraries for file handling, pathing, and time. Since we are importing functions from other parts of the project, for example `src/data_acquisition.py`, we need to add the root directory to the current scope of imports of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fea6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with the handle 'kurzgesagt'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "\n",
    "# load project directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.data_acquisition import *\n",
    "from src.utils import *\n",
    "from paths import Paths\n",
    "import config\n",
    "\n",
    "# show Channel Handle\n",
    "print(f\"Working with the handle '{config.channel_handle}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b8fdc",
   "metadata": {},
   "source": [
    "Some of the generated files will have a name like `file_25_08_01.ndjson`, for that we have provided with a `Paths` class that handles the naming for us. We pass the channel handle, the date object, and the base directory:\n",
    "\n",
    "**1) channel_handle**:\n",
    "> This doens't have to be necessarily the same as the channel handle, it is for naming conventions only.\n",
    "\n",
    "**2) date_obj**:\n",
    "> Some files are generated by days, if the process of extraction takes multiple days the program will generate multiple files. It takes today by default, but a specific time can be passed like `date_path = date(25, 8, 1)`.\n",
    "\n",
    "**3) base_dir**:\n",
    ">You can specify the directory to save the files to. By default points to the root directory of the current project. All files will be saved to `base_dir/data/raw` for raw files and `base_dir/data/processed` for processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ee12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_path = date.today()\n",
    "channel_paths = Paths(channel_handle=config.channel_handle, date_obj=date_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2607bbc",
   "metadata": {},
   "source": [
    "## 2) Extract YouTube Channel ID based on handle\n",
    "Most of the API calls use the Channel ID to complete the queries, so we will need to make a request to the YouTube Data API v3 to get it. The Channel Id for a single channel is an alphanumeric string. For example, for the Channel handle `@smalin` is `UC2zb5cQbLabj3U9l3tke1pg`. The channel handle appears with a `@` symbol in the youtube profile. You can also make a browser search at `youtube.com/@smalin` to confirm that it is indeed the handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8900149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Channel ID for the YouTube handle 'smalin' is UC2zb5cQbLabj3U9l3tke1pg.\n"
     ]
    }
   ],
   "source": [
    "channel_id = get_channel_id(config.channel_handle)\n",
    "print(f\"The Channel ID for the YouTube handle '{config.channel_handle}' is {channel_id}.\")\n",
    "# channel_id = \"UC2zb5cQbLabj3U9l3tke1pg\" # smalin\n",
    "\n",
    "# UCmAOuA1OIofz5XYzARmKkfQ for yeju\n",
    "# UCsXVk37bltHxD1rDPwtNM8Q for Kurzgesagt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23384f8",
   "metadata": {},
   "source": [
    "## 3)  Extract YouTube Uploads Playlist based on Channel ID\n",
    "To get all the uploaded (public) videos that the channel has uploaded, we need to fetch the Uploads Playlist from the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f32d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Uploads Playlist for the YouTube handle 'smalin' is UU2zb5cQbLabj3U9l3tke1pg.\n"
     ]
    }
   ],
   "source": [
    "uploads_playlist_id = get_channel_uploads_playlist(channel_id)\n",
    "print(f\"The Uploads Playlist for the YouTube handle '{config.channel_handle}' is {uploads_playlist_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93143b67",
   "metadata": {},
   "source": [
    "For `smalin`, the uploads playlist was `UU2zb5cQbLabj3U9l3tke1pg`. You can also compare against the YouTube Channel ID `UC2zb5cQbLabj3U9l3tke1pg`:\n",
    "- `UU2zb5cQbLabj3U9l3tke1pg`\n",
    "- `UC2zb5cQbLabj3U9l3tke1pg`\n",
    "\n",
    "They are almost the same string, but the Channel ID starts with `UC` and the Uploads Playlist starts with `UU`. If you want to avoid calling this method, you can modiy the YouTube Channel ID manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61984c",
   "metadata": {},
   "source": [
    "## 4)  Extract YouTube Videos based on YouTube Playlist ID\n",
    "We will save the videos for the given playlist (by default the Uploads playlist) to a json file. The path that will be used is `base_dir/data/raw/<channel_handle>_videos.json`. This file will contain all videos with a format like this:\n",
    "```json\n",
    "[{\n",
    "    \"videoId\": \"21hbsC7nYwk\",\n",
    "    \"done\": false,\n",
    "    \"nextPageToken\": null\n",
    "}, ...],\n",
    "```\n",
    "\n",
    "This structure will be useful later if we have to distribute data extraction across multiple days. If the YouTube is exceeded we have to continue until our quota is reset. Videos completely processed will be marked as **done**, and videos that started processing and did not finish will save the nextPageToken, so that the next day of extraction we can resume where we left off.\n",
    "\n",
    "If the file already exists the function will print an error. If you want to overwrite the file you can set the `overwrite` argument to `True` <span style=\"color: red; font-weight: bold;\">⚠️BUT BE CAREFUL, THIS WILL COMPLETELY ERASE ANY EXTRACTION PROGRESS</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File 'e:\\Conda-Projects\\Jupyter notebooks\\youtube-nlp\\data\\raw\\smalin_videos.json' already exists. Set overwrite=True to allow overwriting.\n"
     ]
    }
   ],
   "source": [
    "# uploads_playlist_id = \"UUsXVk37bltHxD1rDPwtNM8Q\" # for kurzgesagt\n",
    "save_playlist_videos(uploads_playlist_id, channel_paths.videos_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5e31d",
   "metadata": {},
   "source": [
    "## 5)  Extract YouTube Comments & Replies based on YouTube Video List\n",
    "Now for the big part, extracting comments. The function doesn't take any IDs this time, **THIS FUNCTION WORKS FROM THE `data/raw/<channel_handle>_videos.json` FILE**, if no such file exists, you will get an error, the `<channel_handle>_videos.json` is strictly necessary for this function to work. If you want comments for a single video you can create the file at the specified path with the following structure:\n",
    "\n",
    "```json\n",
    "[{\n",
    "    \"videoId\": \"<your_video_id>\",\n",
    "    \"done\": false,\n",
    "    \"nextPageToken\": null\n",
    "}]\n",
    "```\n",
    "\n",
    "You can get the Video ID by going at the desired video in the browser and look at the adress: `youtube.com/watch?v=21hbsC7nYwk`, the Video ID is `21hbsC7nYwk`, the part that comes after `/watch?v=` and before any `&`. Sometimes the adress will contain things like `watch?v=21hbsC7nYwk&list=RD21hbsC7nYwk&start_radio=1&ab_channel=smalin`. The Video ID is still the same, the `...&list=RD21hb...` part is just more arguments of the URL, but the Video ID is still the same.\n",
    "\n",
    "\n",
    "This function will check against every video in `<channel_handle>_videos.json` and extract comments from all the videos that have `\"done\": false`. When a video is done, it is marked as `\"done\": true`. If a video is stopped, either because of an error, or because quota exceeded, the `nextPageToken` will be saved, to give the program indications where to continue from. This is useful when the YouTube video has numerous comments, and a single run is not enough.\n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "The `nextPageToken` is an obscure ID, meaning that it doesn't store any meaningful information, it is used by YouTube internally in order to handle pagination. <span style=\"color: red; font-weight: bold;\">BUT IT CAN CHANGE</span>. A `nextPageToken` will not be valid forever, and it is better not to wait too much time between runs. If you could not finish a video today, plan to continue tomorrow, because in two days it may not be the same. **IF A VIDEO `nextPageToken` BECOMES INVALID YOU HAVE TO REPROCESS THE ENTIRE VIDEO**, this is a limitation of the api, that unfortunately we have to work around on our end. A `nextPageToken` can change for numerous reasons, when a person post a new comment, it can cause the comments to shift, and there may be a need for a new set of pagination tokens. It is not confirmed that the `nextPageToken` changes for this specific reason, just have in mind that **IT CAN CHANGE**. A `nextPageToken` may become invalid in the middle of a fetch, not just across days.\n",
    "\n",
    "\n",
    "For now, if you have page errors, you can go to the `<channel_handle>_videos.json` file and set `nextPageToken` to null and make sure that `done` is set to false:\n",
    "```json\n",
    "[{\n",
    "    \"videoId\": \"<video_id>\",\n",
    "    \"done\": false, // this should remain in false\n",
    "    \"nextPageToken\": \"<next_page_token>\" // set this to null\n",
    "}]\n",
    "```\n",
    "\n",
    "A functionality to handle this issue programmatically is planned for the future.\n",
    "\n",
    "### LOGGING\n",
    "The function by default will log every video. This is convenient when a single video may contain numerous comments. If you want to control how many videos to wait to log, you can use the `log_every_count` argument. If the videos have numerous comments, it is better to leave it at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2282ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Comments fetch initialized...\n",
      "INFO:root:Videos list from e:\\Conda-Projects\\Jupyter notebooks\\youtube-nlp\\data\\raw\\kurzgesagt_videos.json loaded successfully.\n",
      "INFO:root:Video gtDKKJq9u30 processed. Finished: True. Comments: 7323, Replies: 1658\n",
      "INFO:root:Success. Skipped: 301, Processed: 1, Comments: 7323, Replies: 1658. (46.41s)\n",
      "INFO:root:Videos saved successfully, videos count: 1, comments & replies: 8981\n"
     ]
    }
   ],
   "source": [
    "save_all_videos_comments(channel_paths.videos_file_path, channel_paths.raw_comments_file_path, debugging=False, log_every_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bd8a8",
   "metadata": {},
   "source": [
    "We can check the progress for our videos JSON with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa109c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_videos_progress' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mget_videos_progress\u001b[49m(channel_paths\u001b[38;5;241m.\u001b[39mvideos_file_path)\n\u001b[0;32m      2\u001b[0m result\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_videos_progress' is not defined"
     ]
    }
   ],
   "source": [
    "result = get_videos_progress(channel_paths.videos_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d173fdb",
   "metadata": {},
   "source": [
    "### Add a video manually\n",
    "You can add a video manually having the YouTube video ID. You pass the videoId to the function and the videos.json file that you want to append it to, and the function will handle the insertion for you, no need to modify the file manually. Be careful, as adding a video will add any video from any channel, since it doens't check if the video belongs to the current channel. USE IT AT YOUR OWN RISK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94754129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Success: Video gtDKKJq9u30 added.\n"
     ]
    }
   ],
   "source": [
    "add_video(channel_paths.videos_file_path, 'gtDKKJq9u30')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df2f4f",
   "metadata": {},
   "source": [
    "## 6)  (Optional) Extract YouTube Playlists based on Channel ID\n",
    "Since we later can extract a video list for a specific playlist, we can fetch all the youtube playlists for the specified YouTube Channel ID. You can look it up in the URL of a youtube playlist, or a video that is being played as a part of a playlist. The playlist in the url is identified by `list=` and is followed by something like `PLFB3B01978B3A6D7C` (lately they look like `PLtj_HurkS7Zx9aPdiCyf9GI5gC7qsmLwF`), it always starts with `PL`. Remember not to include anything after `&` symbol, excluding the symbol itself.\n",
    "\n",
    "This function will fetch **all channel playlists**, all the playlists that the user created, it doesn't list the Uploads Playlist (it is not user created). It will output all playlists to a json file to the path `base_dir/data/raw/<channel_handle>_playlists.json`. It will look something like this:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"PLFB3B01978B3A6D7C\",\n",
    "        \"title\": \"My Top Videos\",\n",
    "        \"description\": \"\"\n",
    "    },\n",
    "    ...\n",
    "],\n",
    "```\n",
    "\n",
    "From this file you can look up the Playlist ID that you want to extract comments from. Remember that you have to create the `videos.json` for that playlist, and then extract comments from that specific videos file. This function is mostly informational, if the reader wishes to see from a specific playlist, it doesn't contribute anything significant to the main flow of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edc441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_channel_playlists(channel_id, channel_paths.playlists_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0d2d2",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "- Q: **I've got a quotaExceeded error but have not used 10000 units today**.\n",
    "\n",
    "A: Sometimes happens. sometimes would trigger quotaExceeded at 9897 units. The notebook once gets a quotaExceeded finishes\n",
    "processing and saves the progress. From here it is recommended to continue on a different day.\n",
    "\n",
    "- **How do I distinguish between the videos that have comments disabled?**\n",
    "\n",
    "A: With the endpoinds that the notebook uses, there is no way to tell which videos have comments disabled. If a video\n",
    "has comments disabled it will be shown as an error, but the progress will resume for the rest of the videos.\n",
    "\n",
    "- **Why is there so little information about the video itself?**\n",
    "\n",
    "A: The original scope of this project was about YouTube comments NLP, so we only use the YouTube endpoints for comments not for video metadata. It\n",
    "could be added to the project but that would change the scope of the project significantly, since using the endpoints to retrieve video metadata cost\n",
    "significantly more units per request.\n",
    "\n",
    "- **How much content can I extract with the basic quota of 10000 units?**\n",
    "\n",
    "A: With the tests done during the development of this project, the notebook can extract about 300.000 comments per 10.000 units. There isn't an exact number and is hard to estimate since it depends on how much comments there are and how much replies there are for these comments. The notebook doesn't use the replies endpoint if the top level comment has a number of replies of 5 or less. If it has more, it makes a call to the replies endpoint.\n",
    "\n",
    "- **How much time takes the notebook to finish?**\n",
    "\n",
    "A: With the tests done during development, we ran the notebook for about 1 hour every day, point at which the units will be almost expended.\n",
    "\n",
    "- **Does the notebook handles new comments?**\n",
    "\n",
    "A: If the video has not been processed, yes. The notebook doesn't distinguish new comments, the notebook will fetch all comments up to the day that the notebook is run for that video. If the video has been processed, and a new comment gets posted, the notebook doesn't account for the new comment.\n",
    "\n",
    "- **Does the notebook handles new videos?**\n",
    "\n",
    "A: No, You would have to get a new list of videos, but getting a new list of videos will erase any extraction process. The notebook may have a way to handle this case in the future. For now, if you want to add a video, you can do it modifying the file manually, respecting the required format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
