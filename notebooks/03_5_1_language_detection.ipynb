{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b00679",
   "metadata": {},
   "source": [
    "# Language \n",
    "We are using `langdetect` for the task of language detection. Langdetect is not the most accurate but it is lightweight and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469a4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import date\n",
    "import polars as pl\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import config\n",
    "from paths import Paths\n",
    "from src.lang_detect import detect_parallel\n",
    "\n",
    "# change this for a dataset that you have\n",
    "date_obj = date(2025, 9, 10)\n",
    "channel_paths = Paths(channel_handle=config.channel_handle, date_obj=date_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd9c53",
   "metadata": {},
   "source": [
    "## Adding the lang column to the clean dataset\n",
    "We will be working with the clean dataset, since we need the original comment. Additionally, we are performing this process per dataset, that is, if the clean file is from the first of the month, only that file will be updated, so you have to run this notebook for every dataset of each day that you have. It is done this way since it is a relatively slow progress, and managing all datasets in a single go becomes inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f23700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300508 comments loaded.\n"
     ]
    }
   ],
   "source": [
    "comments = pl.read_parquet(channel_paths.clean_comments_file_path, columns=[\"comment\"])[\"comment\"].to_list()\n",
    "print(f\"{len(comments)} comments loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a4acc",
   "metadata": {},
   "source": [
    "This is a task that runs in parallel, making use of the multiple processors in your cpu.\n",
    "\n",
    "Take a look at `03_language_detection_notes.ipynb` for some testing, and the evaluation on how many processors is the sweet spot for your system.\n",
    "\n",
    "This is quite a lengthy process, so feel free to go for a coffee ‚òï and grab some cookies üç™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d385c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c825103b222a4775828f08b5598b042c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished translation in 314.50s, 955.51 c/s\n"
     ]
    }
   ],
   "source": [
    "langs = detect_parallel(comments, max_workers=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8321a5",
   "metadata": {},
   "source": [
    "Somme quick inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa3e61cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 242130),\n",
       " ('und', 35711),\n",
       " ('de', 2577),\n",
       " ('so', 1759),\n",
       " ('af', 1282),\n",
       " ('fr', 1193),\n",
       " ('tl', 1193),\n",
       " ('es', 1157),\n",
       " ('it', 1138),\n",
       " ('id', 1105)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(langs).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106871a3",
   "metadata": {},
   "source": [
    "We append the results to our clean comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file\n",
    "df = pl.read_parquet(channel_paths.clean_comments_file_path)\n",
    "\n",
    "# Add the new 'lang' column (make sure 'lang' is a list or iterable)\n",
    "df = df.with_columns(pl.Series(\"lang\", langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write back to the parquet file (overwrite)\n",
    "df.write_parquet(channel_paths.clean_comments_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe3f84",
   "metadata": {},
   "source": [
    "# Q & A\n",
    "\n",
    "- **The process was very slow, are there any other language detectors available?**\n",
    "\n",
    "A: Multiple language detectors were tried.\n",
    "1) Langid has not been update in quite some time, and in linux would behave unexpectedly, it would span multiple threads just by importing the library.\n",
    "2) Fasttext has a complex installation on windows.\n",
    "3) Lingua was extremelly fast, but the way that it handles confidence values across multiple languages was found unsatisfactory.\n",
    "4) pycld3 like fastttext has a complex installation on windows.\n",
    "\n",
    "langdetect had some issues with accuracy and speed, but was finally chosen by its simplicity and lightweightness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
