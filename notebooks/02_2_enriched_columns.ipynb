{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4053d7c9",
   "metadata": {},
   "source": [
    "# Enriched Columns\n",
    "In this section we will derive some columns but these will be different in the sence that are transformations of the \"comment\" column of the clean dataset. They will be useful for some analyses that we will run in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c21b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with the channel handle: kurzgesagt.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datetime import date\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "# load project directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.preprocessing import *\n",
    "from paths import Paths\n",
    "import config\n",
    "\n",
    "print(f\"Working with the channel handle: {config.channel_handle}.\")\n",
    "date_today = date.today()\n",
    "channel_paths = Paths(channel_handle=config.channel_handle, date_obj= date_today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aace777",
   "metadata": {},
   "source": [
    "# Derived columns\n",
    "We are derivating the following columns from the comment:\n",
    "- Tokens simple\n",
    "- Tokens without stopwords\n",
    "- Emojis only\n",
    "- Mentions (that start with @, eg, @username)\n",
    "- Hashtags, although rare for YouTube comments\n",
    "\n",
    "The functions to get all these derived columns are over in the `src/preprocessing.py` file. There you can peek to learn more about what each function is doing in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7438cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tokens_simple(df: pl.DataFrame) -> pl.Series:\n",
    "    return df[\"comment\"].map_elements(lambda t: tokenize_mixed(t, keep_stopwords=True))\n",
    "\n",
    "def add_tokens_wo_stop(df: pl.DataFrame) -> pl.Series:\n",
    "    return df[\"comment\"].map_elements(lambda t: tokenize_mixed(t, keep_stopwords=False))\n",
    "\n",
    "def add_emojis(df: pl.DataFrame) -> pl.Series:\n",
    "    return df[\"comment\"].map_elements(lambda t: extract_emojis(t))\n",
    "\n",
    "def add_mentions(df: pl.DataFrame) -> pl.Series:\n",
    "    return df['comment'].map_elements(lambda t: extract_mentions(t))\n",
    "\n",
    "def add_hashtags(df: pl.DataFrame) -> pl.Series:\n",
    "    return df['comment'].map_elements(lambda t: extract_hashtags(t))\n",
    "\n",
    "# def add_tokens_lemmatized(df: pl.DataFrame) -> pl.Series:\n",
    "#     return df[\"comment\"].map_elements(lambda t: lemmatize_tokens(tokenize_mixed(t)))\n",
    "\n",
    "ENRICHERS = {\n",
    "    \"tokens_simple\": add_tokens_simple,\n",
    "    \"tokens_wo_stop\": add_tokens_wo_stop,\n",
    "    \"emojis\": add_emojis,\n",
    "    \"mentions\": add_mentions,\n",
    "    \"hashtags\": add_hashtags\n",
    "    # \"tokens_lemmatized\": add_tokens_lemmatized,\n",
    "    # ... add as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5976f6",
   "metadata": {},
   "source": [
    "## Future enriched columns\n",
    "In the case that a new column is added to the enriched process, the following function provides a way to add or patch columns, that is, it will not touch columns already\n",
    "in the dataset, and only add those columns that are new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd04aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_patch_columns(df: pl.DataFrame, enrichers: dict):\n",
    "    \n",
    "    # Figure out which enrichers are missing\n",
    "    missing = [col for col in enrichers if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Adding '{missing}'\")\n",
    "    else:\n",
    "        print(\"All columns present\")\n",
    "        return df\n",
    "\n",
    "    # Apply only the missing enrichers\n",
    "    for col in tqdm(missing, desc=\"Adding missing columns\", unit=\"col\"):\n",
    "        df = df.with_columns([\n",
    "            enrichers[col](df).alias(col)\n",
    "        ])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4078211",
   "metadata": {},
   "source": [
    "# Reading from the clean comments file.\n",
    "Every enriched file will have a similar partitioning to those of clean comments. It will be sepparated by days, and every clean file for a specific day will generate a new enriched parquet file with all the desired derived columns. For now, we will need the original comment from the clean comments parquet, we will save also the ID of the comment to identify every derived column with its original comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e87b780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comment_id</th><th>comment</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Ugzwc2k1UTzpSCw1wq54AaABAg.8fZâ€¦</td><td>&quot;Lucian MacAndrew&nbsp;&nbsp;the holy Qurâ€¦</td></tr><tr><td>&quot;UgixBTWBLENWT3gCoAEC.80UF8ZksDâ€¦</td><td>&quot;007VitaminD Murder is bad, sexâ€¦</td></tr><tr><td>&quot;UgivQIBkqYD753gCoAEC&quot;</td><td>&quot;Kurzgesagt, you da man&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ comment_id                      â”† comment                         â”‚\n",
       "â”‚ ---                             â”† ---                             â”‚\n",
       "â”‚ str                             â”† str                             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Ugzwc2k1UTzpSCw1wq54AaABAg.8fZâ€¦ â”† Lucian MacAndrew  the holy Qurâ€¦ â”‚\n",
       "â”‚ UgixBTWBLENWT3gCoAEC.80UF8ZksDâ€¦ â”† 007VitaminD Murder is bad, sexâ€¦ â”‚\n",
       "â”‚ UgivQIBkqYD753gCoAEC            â”† Kurzgesagt, you da man          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(channel_paths.clean_comments_file_path, columns=['comment_id', 'comment'])\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e156e",
   "metadata": {},
   "source": [
    "Running the function that adds the derived columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa813f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding '['tokens_simple', 'tokens_wo_stop', 'emojis', 'mentions', 'hashtags']'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding missing columns: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:43<00:00,  8.61s/col]\n"
     ]
    }
   ],
   "source": [
    "df = add_or_patch_columns(df, ENRICHERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09675e",
   "metadata": {},
   "source": [
    "This is how our dataset looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9594b207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>comment_id</th><th>comment</th><th>tokens_simple</th><th>tokens_wo_stop</th><th>emojis</th><th>mentions</th><th>hashtags</th></tr><tr><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;UgwVii-xFX3XsVM6YPh4AaABAg&quot;</td><td>&quot;Looks like a subnautica fishğŸ˜…&quot;</td><td>[&quot;looks&quot;, &quot;like&quot;, â€¦ &quot;fish&quot;]</td><td>[&quot;looks&quot;, &quot;like&quot;, â€¦ &quot;fish&quot;]</td><td>[&quot;ğŸ˜…&quot;]</td><td>[]</td><td>[]</td></tr><tr><td>&quot;Ugh_qztkNYvDTngCoAEC.8JjzTsy9qâ€¦</td><td>&quot;@@Chivas6 But human beings areâ€¦</td><td>[&quot;chivas&quot;, &quot;but&quot;, â€¦ &quot;existence&quot;]</td><td>[&quot;chivas&quot;, &quot;human&quot;, â€¦ &quot;existence&quot;]</td><td>[]</td><td>[&quot;@Chivas6&quot;]</td><td>[]</td></tr><tr><td>&quot;Ugx1fZD5ySCw7dOdMx54AaABAg.9UHâ€¦</td><td>&quot;@@Jesuisunknown in the mighty â€¦</td><td>[&quot;jesuisunknown&quot;, &quot;in&quot;, â€¦ &quot;lol&quot;]</td><td>[&quot;jesuisunknown&quot;, &quot;mighty&quot;, â€¦ &quot;lol&quot;]</td><td>[]</td><td>[&quot;@Jesuisunknown&quot;]</td><td>[]</td></tr><tr><td>&quot;Ugw9BBNy2rh0s0pcsUx4AaABAg&quot;</td><td>&quot;I didn&#x27;t see that coming 4:16&quot;</td><td>[&quot;i&quot;, &quot;didn&quot;, â€¦ &quot;coming&quot;]</td><td>[&quot;see&quot;, &quot;coming&quot;]</td><td>[]</td><td>[]</td><td>[]</td></tr><tr><td>&quot;Ugwy_-dij3dYe6JWufp4AaABAg.9F8â€¦</td><td>&quot;@@rashidahmad3086 the book of â€¦</td><td>[&quot;rashidahmad&quot;, &quot;the&quot;, â€¦ &quot;that&quot;]</td><td>[&quot;rashidahmad&quot;, &quot;book&quot;, â€¦ &quot;way&quot;]</td><td>[]</td><td>[&quot;@rashidahmad3086&quot;]</td><td>[]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ comment_id   â”† comment      â”† tokens_simpl â”† tokens_wo_st â”† emojis    â”† mentions     â”† hashtags  â”‚\n",
       "â”‚ ---          â”† ---          â”† e            â”† op           â”† ---       â”† ---          â”† ---       â”‚\n",
       "â”‚ str          â”† str          â”† ---          â”† ---          â”† list[str] â”† list[str]    â”† list[str] â”‚\n",
       "â”‚              â”†              â”† list[str]    â”† list[str]    â”†           â”†              â”†           â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ UgwVii-xFX3X â”† Looks like a â”† [\"looks\",    â”† [\"looks\",    â”† [\"ğŸ˜…\"]    â”† []           â”† []        â”‚\n",
       "â”‚ sVM6YPh4AaAB â”† subnautica   â”† \"like\", â€¦    â”† \"like\", â€¦    â”†           â”†              â”†           â”‚\n",
       "â”‚ Ag           â”† fishğŸ˜…       â”† \"fish\"]      â”† \"fish\"]      â”†           â”†              â”†           â”‚\n",
       "â”‚ Ugh_qztkNYvD â”† @@Chivas6    â”† [\"chivas\",   â”† [\"chivas\",   â”† []        â”† [\"@Chivas6\"] â”† []        â”‚\n",
       "â”‚ TngCoAEC.8Jj â”† But human    â”† \"but\", â€¦     â”† \"human\", â€¦   â”†           â”†              â”†           â”‚\n",
       "â”‚ zTsy9qâ€¦      â”† beings areâ€¦  â”† \"existenceâ€¦  â”† \"existenâ€¦    â”†           â”†              â”†           â”‚\n",
       "â”‚ Ugx1fZD5ySCw â”† @@Jesuisunkn â”† [\"jesuisunkn â”† [\"jesuisunkn â”† []        â”† [\"@Jesuisunk â”† []        â”‚\n",
       "â”‚ 7dOdMx54AaAB â”† own in the   â”† own\", \"in\",  â”† own\",        â”†           â”† nown\"]       â”†           â”‚\n",
       "â”‚ Ag.9UHâ€¦      â”† mighty â€¦     â”† â€¦ \"lolâ€¦      â”† \"mighty\", â€¦  â”†           â”†              â”†           â”‚\n",
       "â”‚              â”†              â”†              â”† â€¦            â”†           â”†              â”†           â”‚\n",
       "â”‚ Ugw9BBNy2rh0 â”† I didn't see â”† [\"i\",        â”† [\"see\",      â”† []        â”† []           â”† []        â”‚\n",
       "â”‚ s0pcsUx4AaAB â”† that coming  â”† \"didn\", â€¦    â”† \"coming\"]    â”†           â”†              â”†           â”‚\n",
       "â”‚ Ag           â”† 4:16         â”† \"coming\"]    â”†              â”†           â”†              â”†           â”‚\n",
       "â”‚ Ugwy_-dij3dY â”† @@rashidahma â”† [\"rashidahma â”† [\"rashidahma â”† []        â”† [\"@rashidahm â”† []        â”‚\n",
       "â”‚ e6JWufp4AaAB â”† d3086 the    â”† d\", \"the\", â€¦ â”† d\", \"book\",  â”†           â”† ad3086\"]     â”†           â”‚\n",
       "â”‚ Ag.9F8â€¦      â”† book of â€¦    â”† \"thatâ€¦       â”† â€¦ \"wayâ€¦      â”†           â”†              â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9c51d",
   "metadata": {},
   "source": [
    "Polars file size estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67483ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.32 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df.estimated_size()/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df2081",
   "metadata": {},
   "source": [
    "## Saving the results back to parquet\n",
    "Now that we have the enrichers in place, we are ready to save our results in a parquet file. These files will go into the `data/processed/enriched` file, and will be identified by the channel handle and the date from the clean parquet file. For example:\n",
    "- `comments/kurzgesagt_comments_2025_09_03.parquet` will become `enriched/kurzgesagt_enriched_comments_2025_09_03.parquet`.\n",
    "We won't save the original comment in the enriched file, it can be fetched from the clean comments file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4042658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = {\"comment\", }\n",
    "df = df.select([col for col in df.columns if col not in cols_to_exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1492b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(channel_paths.enriched_comments_file_path, compression='zstd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23601706",
   "metadata": {},
   "source": [
    "# Bulk add columns\n",
    "The following sections was made when a new column is added to the analysis and needs to be added to the existing enriched files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8964510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in channel_paths.list_enriched_files():\n",
    "#     print(f'Checking file at {file}')\n",
    "#     df = pl.read_parquet(file)\n",
    "#     df = add_or_patch_columns(df, enrichers=ENRICHERS)\n",
    "#     df.write_parquet(channel_paths.enriched_comments_file_path, compression='zstd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
